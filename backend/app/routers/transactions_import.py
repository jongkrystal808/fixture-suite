# /backend/app/routers/transactions_import.py

"""
äº¤æ˜“ Excel åŒ¯å…¥ Routerï¼ˆæ”¶ / é€€ åˆä½µç‰ˆ v6ï½œSAFEï¼‰
- ä¾ transaction_type åˆ†æµ receipt / return
- SP-firstï¼ˆv6ï¼‰
- validate-firstï¼Œä¸ç•™ä¸‹å¤±æ•—äº¤æ˜“
- âœ… datecode å®Œå…¨ä¸è®€ serialsï¼ˆé¿å… NaN -> "nan"ï¼‰
- âœ… batch æ”¯æ´ serial_start ~ serial_end å€é–“å±•é–‹
- âœ… æ”¯æ´é›™ Sheetï¼šSerial_Transactions / Datecode_Transactions
- âœ… æ”¯æ´ã€Œä¸Šæ–¹èªªæ˜å€ã€ï¼šè‡ªå‹•åµæ¸¬è¡¨é ­åˆ—
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
import pandas as pd
from io import BytesIO
import pymysql
import re
from typing import Optional

from backend.app.database import db
from backend.app.dependencies import get_current_user, get_current_customer_id

router = APIRouter(
    prefix="/transactions",
    tags=["Transactions Import"]
)

# ============================================================
# Config
# ============================================================
MAX_BATCH_SIZE = 2000
WARNING_BATCH_SIZE = 1000  # è¶…éæ­¤æ•¸é‡åªè­¦å‘Šï¼Œä¸é˜»æ“‹


# ============================================================
# Helperï¼ˆå…±ç”¨ï¼‰
# ============================================================

def ensure_fixture_exists(fixture_id: str, customer_id: str):
    row = db.execute_query(
        "SELECT id FROM fixtures WHERE id=%s AND customer_id=%s",
        (fixture_id, customer_id)
    )
    if not row:
        raise ValueError(f"æ²»å…· {fixture_id} ä¸å­˜åœ¨æˆ–ä¸å±¬æ–¼æ­¤å®¢æˆ¶")


def normalize_source_type(val) -> str:
    v = ("" if val is None or (isinstance(val, float) and pd.isna(val)) else str(val)).strip().lower()
    if v in ("self_purchased", "è‡ªè³¼"):
        return "self_purchased"
    if v in ("customer_supplied", "å®¢ä¾›", "customer supplied"):
        return "customer_supplied"
    raise ValueError("source_type ä¸åˆæ³•ï¼ˆå…è¨±ï¼šself_purchased / customer_suppliedï¼‰")


def normalize_tx_type(val) -> str:
    v = ("" if val is None or (isinstance(val, float) and pd.isna(val)) else str(val)).strip().lower()
    if v in ("receipt", "return"):
        return v
    raise ValueError("transaction_type å¿…é ˆç‚º receipt æˆ– return")


def normalize_record_type(val) -> str:
    v = ("" if val is None or (isinstance(val, float) and pd.isna(val)) else str(val)).strip().lower()
    if v in ("batch", "individual", "datecode"):
        return v
    raise ValueError("record_type ä¸åˆæ³•ï¼ˆå…è¨±ï¼šbatch / individual / datecodeï¼‰")


def read_str_optional(val) -> Optional[str]:
    if val is None:
        return None
    if isinstance(val, float) and pd.isna(val):
        return None
    s = str(val).strip()
    return s or None


def normalize_order_no(val):
    if val is None:
        return None
    if isinstance(val, float) and pd.isna(val):
        return None

    s = str(val).strip()
    if not s:
        return None

    # Excel å¸¸è¦‹ 25123456.0
    if s.endswith(".0"):
        s = s[:-2]

    return s


# ============================================================
# ğŸ”¥ æ–°å¢ï¼šå±•é–‹ batch å€é–“
# ============================================================

def expand_serial_range(start: str, end: str) -> list[str]:
    """
    å±•é–‹ SN001 ~ SN100 é¡å‹å€é–“
    """
    pattern = r"([A-Za-z\-]*)(\d+)$"

    m1 = re.match(pattern, start)
    m2 = re.match(pattern, end)

    if not m1 or not m2:
        raise ValueError("åºè™Ÿæ ¼å¼éŒ¯èª¤ï¼Œç„¡æ³•å±•é–‹å€é–“")

    prefix1, num1 = m1.groups()
    prefix2, num2 = m2.groups()

    if prefix1 != prefix2:
        raise ValueError("serial_start èˆ‡ serial_end å‰ç¶´ä¸åŒ")

    width = len(num1)
    start_num = int(num1)
    end_num = int(num2)

    if start_num > end_num:
        raise ValueError("åºè™Ÿå€é–“éŒ¯èª¤ï¼ˆstart > endï¼‰")

    return [
        f"{prefix1}{str(i).zfill(width)}"
        for i in range(start_num, end_num + 1)
    ]


def parse_serials_csv(raw_serials) -> tuple[str, int]:
    if raw_serials is None:
        raise ValueError("serials ä¸å¯ç‚ºç©ºï¼ˆindividual å¿…å¡«ï¼‰")
    if isinstance(raw_serials, float) and pd.isna(raw_serials):
        raise ValueError("serials ä¸å¯ç‚ºç©ºï¼ˆindividual å¿…å¡«ï¼‰")

    if not isinstance(raw_serials, str):
        raw_serials = str(raw_serials)

    raw = raw_serials.strip()
    if not raw:
        raise ValueError("serials ä¸å¯ç‚ºç©ºï¼ˆindividual å¿…å¡«ï¼‰")

    serials = [s.strip() for s in raw.split(",") if s.strip()]
    if not serials:
        raise ValueError("serials ç„¡æ•ˆ")

    serials_csv = ",".join(serials)

    if serials_csv.lower() == "nan":
        raise ValueError("serials æ¬„ä½ç•°å¸¸ï¼ˆNaNï¼‰")

    return serials_csv, len(serials)


def parse_datecode_qty(raw_datecode, raw_qty) -> tuple[str, int]:
    if raw_datecode is None or (isinstance(raw_datecode, float) and pd.isna(raw_datecode)):
        raise ValueError("datecode ä¸å¯ç‚ºç©º")

    dc = str(raw_datecode).strip()
    if not dc:
        raise ValueError("datecode ä¸å¯ç‚ºç©º")

    if raw_qty is None or (isinstance(raw_qty, float) and pd.isna(raw_qty)):
        raise ValueError("quantity ä¸å¯ç‚ºç©º")

    # dtype=str è®€å…¥å¾Œï¼Œé€™è£¡å¯èƒ½æ˜¯ "50"
    try:
        qty = int(str(raw_qty).strip())
    except Exception:
        raise ValueError("quantity å¿…é ˆç‚ºæ•´æ•¸")

    if qty <= 0:
        raise ValueError("quantity å¿…é ˆ > 0")

    return dc, qty


# ============================================================
# Excel è®€å–ï¼šæ”¯æ´é›™ Sheet + è‡ªå‹•æ‰¾è¡¨é ­
# ============================================================

def _find_header_row(df_raw: pd.DataFrame, required_cols: list[str]) -> int:
    """
    åœ¨ header=None çš„ df_raw ä¸­æ‰¾åˆ°ã€ŒåŒ…å« required_colsã€çš„é‚£ä¸€åˆ—ä½œç‚ºè¡¨é ­åˆ—
    """
    required = set([c.lower() for c in required_cols])

    for i in range(len(df_raw)):
        row_vals = df_raw.iloc[i].tolist()
        row_norm = set()
        for v in row_vals:
            if v is None or (isinstance(v, float) and pd.isna(v)):
                continue
            s = str(v).replace("*", "").strip().lower()
            if s:
                row_norm.add(s)
        if required.issubset(row_norm):
            return i

    raise ValueError(f"æ‰¾ä¸åˆ°è¡¨é ­åˆ—ï¼ˆå¿…é ˆåŒ…å«æ¬„ä½ï¼š{required_cols}ï¼‰")


def _sheet_to_df(df_raw: pd.DataFrame, required_cols: list[str]) -> pd.DataFrame:
    header_row = _find_header_row(df_raw, required_cols)
    headers = [str(x).strip() for x in df_raw.iloc[header_row].tolist()]

    df = df_raw.iloc[header_row + 1:].copy()
    df.columns = headers
    df = normalize_columns(df)

    # å»æ‰å®Œå…¨ç©ºç™½åˆ—
    df = df.dropna(how="all")

    # çµ±ä¸€æˆå­—ä¸²ï¼Œé¿å… NaN / ç§‘å­¸è¨˜è™Ÿäº‚å…¥
    df = df.fillna("")
    for c in df.columns:
        df[c] = df[c].apply(lambda x: str(x).strip() if x is not None else "")

    return df


def read_transactions_excel(content: bytes) -> pd.DataFrame:
    """
    å›å‚³åˆä½µå¾Œçš„ DataFrame
    - Serial_Transactionsï¼šéœ€è¦ transaction_type / fixture_id / record_type / source_type
    - Datecode_Transactionsï¼šå…è¨±æ²’æœ‰ record_typeï¼Œæœƒè‡ªå‹•è£œæˆ datecode
    """
    xls = pd.read_excel(BytesIO(content), sheet_name=None, header=None, dtype=str)

    merged: list[pd.DataFrame] = []

    for sheet_name, df_raw in xls.items():
        name = str(sheet_name).strip().lower()

        if "datecode" in name:
            # datecode sheetï¼šrecord_type å¯ä»¥ä¸æä¾›
            base_required = ["transaction_type", "fixture_id", "source_type", "datecode", "quantity"]
            df = _sheet_to_df(df_raw, required_cols=base_required)

            if "record_type" not in df.columns:
                df["record_type"] = "datecode"
            merged.append(df)

        elif "serial" in name:
            required = ["transaction_type", "fixture_id", "record_type", "source_type"]
            df = _sheet_to_df(df_raw, required_cols=required)
            merged.append(df)

        else:
            # å…¼å®¹å–® sheet èˆŠç¯„æœ¬ï¼šåªè¦æ‰¾åˆ°å¿…è¦æ¬„ä½å°±åƒ
            required = ["transaction_type", "fixture_id", "record_type", "source_type"]
            try:
                df = _sheet_to_df(df_raw, required_cols=required)
                merged.append(df)
            except Exception:
                # ä¸æ˜¯è³‡æ–™ sheet å°±è·³é
                continue

    if not merged:
        raise ValueError("Excel å…§æ‰¾ä¸åˆ°å¯åŒ¯å…¥çš„è³‡æ–™ Sheetï¼ˆå»ºè­°å‘½åï¼šSerial_Transactions / Datecode_Transactionsï¼‰")

    out = pd.concat(merged, ignore_index=True)

    # å»æ‰ transaction_type ç­‰é—œéµæ¬„ä½éƒ½ç©ºçš„åˆ—
    out = out[~((out.get("transaction_type", "") == "") & (out.get("fixture_id", "") == ""))]

    # å¿½ç•¥ç¤ºä¾‹åˆ—
    out = out[
        ~out.apply(
            lambda r: any(
                str(v).lower() in ("example", "ç¤ºä¾‹", "ç¯„ä¾‹", "test")
                for v in r
            ),
            axis=1,
        )
    ]

    return out


# ============================================================
# Excel åŒ¯å…¥ï¼ˆäº¤æ˜“åˆä½µï¼‰
# ============================================================

@router.post("/import", summary="äº¤æ˜“ Excel åŒ¯å…¥ï¼ˆæ”¶ / é€€ åˆä½µï¼‰")
async def import_transactions(
    file: UploadFile = File(...),
    user=Depends(get_current_user),
    customer_id: str = Depends(get_current_customer_id),
):
    operator = user["username"]
    created_by = user["id"]

    if not file.filename or not file.filename.lower().endswith(".xlsx"):
        raise HTTPException(status_code=400, detail="è«‹ä½¿ç”¨ .xlsx æª”æ¡ˆåŒ¯å…¥")

    try:
        content = await file.read()
        df = read_transactions_excel(content)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Excel è®€å–å¤±æ•—ï¼š{e}")

    # base å¿…è¦æ¬„ä½ï¼ˆrecord_type å¯èƒ½ç”± datecode sheet è‡ªå‹•è£œï¼‰
    required_cols = ["transaction_type", "fixture_id", "record_type", "source_type"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise HTTPException(status_code=400, detail=f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing}")

    total_created = 0
    errors: list[str] = []
    warnings: list[str] = []

    for idx, row in df.iterrows():
        row_no = idx + 2  # åˆä½µå¾Œ row_no åƒ…ä½œæç¤ºç”¨
        try:
            tx_type = normalize_tx_type(row.get("transaction_type"))
            fixture_id = read_str_optional(row.get("fixture_id"))
            record_type = normalize_record_type(row.get("record_type"))
            source_type = normalize_source_type(row.get("source_type"))

            if not fixture_id:
                raise ValueError("fixture_id ä¸å¯ç‚ºç©º")

            ensure_fixture_exists(fixture_id, customer_id)

            order_no = normalize_order_no(row.get("order_no"))
            note = read_str_optional(row.get("note"))

            serials_csv = None
            datecode = None
            quantity = None

            # =========================
            # batch
            # =========================
            if record_type == "batch":
                serial_start = read_str_optional(row.get("serial_start"))
                serial_end = read_str_optional(row.get("serial_end"))

                if not serial_start or not serial_end:
                    raise ValueError("batch å¿…é ˆå¡«å¯« serial_start èˆ‡ serial_end")

                serial_list = expand_serial_range(serial_start, serial_end)
                size = len(serial_list)

                # ğŸ”¥ å¤§å€é–“ä¿è­·ï¼ˆä¸Šé™ 2000ï¼‰
                if size > MAX_BATCH_SIZE:
                    raise ValueError(f"batch å€é–“éå¤§ï¼ˆ{size} ç­†ï¼‰ï¼Œä¸Šé™ç‚º {MAX_BATCH_SIZE} ç­†")

                # ğŸ” ç•°å¸¸å€é–“è­¦å‘Šï¼ˆä¸é˜»æ“‹ï¼‰
                if size > WARNING_BATCH_SIZE:
                    warnings.append(f"ç¬¬ {row_no} è¡Œï¼šbatch å€é–“åå¤§ï¼ˆ{size} ç­†ï¼‰ï¼Œè«‹ç¢ºèªæ˜¯å¦ç‚ºé æœŸæ“ä½œ")

                serials_csv = ",".join(serial_list)
                quantity = size
                datecode = None

            # =========================
            # individual
            # =========================
            elif record_type == "individual":
                serials_csv, quantity = parse_serials_csv(row.get("serials"))
                datecode = None

            # =========================
            # datecode
            # =========================
            elif record_type == "datecode":
                datecode, quantity = parse_datecode_qty(
                    row.get("datecode"),
                    row.get("quantity"),
                )
                serials_csv = None

            sp_name = "sp_material_receipt_v6" if tx_type == "receipt" else "sp_material_return_v6"

            out = db.call_sp_with_out(
                sp_name,
                [
                    customer_id,
                    fixture_id,
                    order_no,
                    operator,
                    note,
                    created_by,
                    record_type,
                    source_type,
                    serials_csv,
                    datecode,
                    quantity,
                ],
                ["o_transaction_id", "o_message"],
            )

            if not out or not out.get("o_transaction_id"):
                raise ValueError(out.get("o_message") if out else "äº¤æ˜“å¤±æ•—")

            total_created += 1

        except Exception as e:
            msg = str(e)
            if isinstance(e, pymysql.MySQLError) and getattr(e, "args", None) and len(e.args) >= 2:
                msg = e.args[1]
            errors.append(f"ç¬¬ {row_no} è¡Œï¼š{msg}")

    return {
        "count": total_created,
        "failed": len(errors),
        "errors": errors,
        "warnings": warnings,
    }


def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = (
        df.columns
            .astype(str)
            .str.replace("*", "", regex=False)
            .str.replace("ã€€", "", regex=False)  # å…¨å½¢ç©ºç™½
            .str.strip()
            .str.lower()
    )
    return df
